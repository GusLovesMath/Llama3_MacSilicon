# Meta-Llama-3 on Mac Silicon

## Overview
This Jupyter notebook demonstrates how to run the Meta-Llama-3 model on Apple's Mac silicon devices from [**My Medium Post**](https://medium.com/@guslovesmath/efficiently-running-meta-llama-3-on-mac-silicon-m1-m2-m3-61585c9bc741). It includes examples of generating responses from simple prompts and delves into more complex scenarios like solving mathematical problems.

## Requirements
- Apple Mac with M1, M2, or M3 chip
- macOS Monterey or later
- Python 3.x
- Required Python packages: `ipywidgets`, `torch`, `mlx-lm`

## Setup
Clone this repository and install the necessary packages:

```bash
pip install ipywidgets torch mlx-lm
```

[MLX Cummunity on Hugging Face](https://huggingface.co/mlx-community)
